#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Python 3.10+

"""
Projekt: EvoWorld Bot
Autor: Ing. VÃ¡clav PastuÅ¡ek
Datum: 2025-03-22
Verze: 1.0.0
Licence: MIT
Popis: Tento skript implementuje DDQN (Double Deep Q-Network) agenta pro hru EvoWorld.io.
Agent se uÄÃ­ hrÃ¡t pomocÃ­ posilovanÃ©ho uÄenÃ­ (Reinforcement Learning, RL).
VyuÅ¾Ã­vÃ¡ neuronovou sÃ­Å¥ k odhadu hodnot akÄnÃ­ch strategiÃ­ a kombinuje evaluaÄnÃ­ a target sÃ­Å¥,
coÅ¾ Å™eÅ¡Ã­ problÃ©m pÅ™eceÅˆovÃ¡nÃ­ Q-hodnot, typickÃ½ pro standardnÃ­ DQN.
Agent vnÃ­mÃ¡ hru jako obrazovÃ© vstupy, analyzuje je a provÃ¡dÃ­ akce na zÃ¡kladÄ› predikcÃ­ svÃ© sÃ­tÄ›.
TrÃ©nuje se s vyuÅ¾itÃ­m Replay Bufferu a Target Network Update, aby se zlepÅ¡ila stabilita uÄenÃ­.
CÃ­lem je dosÃ¡hnout optimÃ¡lnÃ­ strategie pÅ™eÅ¾itÃ­ a evoluce v hernÃ­m prostÅ™edÃ­.
"""

# ğŸ› ï¸ StandardnÃ­ knihovny Pythonu
import atexit  # SpustÃ­ funkci pÅ™i ukonÄenÃ­ programu (napÅ™. uklÃ­zenÃ­ prostÅ™edkÅ¯)
import json  # PrÃ¡ce s JSON soubory (uklÃ¡dÃ¡nÃ­ a naÄÃ­tÃ¡nÃ­ dat)
import os  # PÅ™Ã­stup k funkcÃ­m operaÄnÃ­ho systÃ©mu (soubory, promÄ›nnÃ© prostÅ™edÃ­)
import threading  # PrÃ¡ce s vlÃ¡kny pro soubÄ›Å¾nÃ© provÃ¡dÄ›nÃ­ Ãºloh
from _typeshed import SupportsWrite  # TypovÃ½ hint pro objekty, kterÃ© podporujÃ­ zÃ¡pis Å™etÄ›zcÅ¯ (str)
from time import sleep  # Pauza v bÄ›hu programu (ÄasovÃ½ delay)
import random  # GenerovÃ¡nÃ­ nÃ¡hodnÃ½ch ÄÃ­sel a vÃ½bÄ›r prvkÅ¯ ze seznamÅ¯
from collections import deque  # EfektivnÃ­ struktura FIFO/LIFO (fronta/zÃ¡sobnÃ­k)
from typing import Deque, Optional, Dict

# ğŸ”¢ Knihovny pro vÄ›deckÃ© vÃ½poÄty a strojovÃ© uÄenÃ­
import numpy as np  # NumerickÃ© vÃ½poÄty s maticemi a vektory
import tensorflow as tf  # StrojovÃ© uÄenÃ­ a neuronovÃ© sÃ­tÄ›

# âŒ¨ï¸ Interakce s klÃ¡vesnicÃ­ a obrazovkou
import keyboard  # ZachytÃ¡vÃ¡nÃ­ vstupÅ¯ z klÃ¡vesnice a simulace stiskÅ¯ klÃ¡ves
import mss  # RychlÃ© snÃ­mÃ¡nÃ­ obrazovky (screenshoty)

# ğŸŒ Automatizace webovÃ©ho prohlÃ­Å¾eÄe (Selenium)
from selenium import webdriver  # HlavnÃ­ knihovna pro ovlÃ¡dÃ¡nÃ­ prohlÃ­Å¾eÄe
from selenium.webdriver import ActionChains, Keys  # Simulace pohybu myÅ¡i a klÃ¡ves
from selenium.webdriver.chrome.options import Options  # Konfigurace Chromu
from selenium.webdriver.support.wait import WebDriverWait  # ÄŒekÃ¡nÃ­ na naÄtenÃ­ prvkÅ¯
from selenium.webdriver.support import expected_conditions as EC  # OvÄ›Å™ovÃ¡nÃ­ viditelnosti prvkÅ¯
from selenium.webdriver.common.by import By  # HledÃ¡nÃ­ prvkÅ¯ na strÃ¡nce (napÅ™. By.ID, By.XPATH)

# ğŸ“‚ VlastnÃ­ pomocnÃ© funkce (import z vlastnÃ­ho souboru `utils.py`)
from utils import *  # Import vÅ¡ech funkcÃ­ z utils.py (pravdÄ›podobnÄ› vlastnÃ­ pomocnÃ© metody)

# VypnutÃ­ oneDNN pro snÃ­Å¾enÃ­ varovÃ¡nÃ­ pÅ™i inicializaci TensorFlow
os.environ["TF_ENABLE_ONEDNN_OPTS"]: str = "0"

# Cesty k modelÅ¯m (pro evaluaci a trÃ©novÃ¡nÃ­)
EVAL_MODEL_PATH: str = "eval_model.keras"
TARGET_MODEL_PATH: str = "target_model.keras"
EPSILON_PATH: str = 'epsilon.json'

# MoÅ¾nÃ© akce pro agenta (klÃ¡vesy, kterÃ© mÅ¯Å¾e stisknout)
ACTION_MAP: dict[int, str | list[str]] = {
    0: "a",  # Pohyb doleva
    1: "w",  # Pohyb nahoru
    2: "d",  # Pohyb doprava
    3: "space",  # Skok
    4: ["a", "w"],  # Pohyb doleva + nahoru
    5: ["d", "w"],  # Pohyb doprava + nahoru
    6: "wait"  # ÄŒekÃ¡nÃ­ (100ms neprovÃ¡dÃ­ akci)
}

# PoÄet dostupnÃ½ch akcÃ­
ACTION_LEN: int = len(ACTION_MAP)

# Å˜Ã­zenÃ­ trÃ©novÃ¡nÃ­
is_training: bool = False  # Indikuje, zda prÃ¡vÄ› probÃ­hÃ¡ trÃ©novÃ¡nÃ­
train_lock: threading.Lock = threading.Lock()  # ZÃ¡mek pro synchronizaci trÃ©novÃ¡nÃ­
can_train: bool = False  # OvlÃ¡dÃ¡nÃ­ spuÅ¡tÄ›nÃ­ trÃ©novÃ¡nÃ­

# Hyperparametry pro trÃ©novÃ¡nÃ­ agenta
GAMMA: float = 0.99  # DiskontnÃ­ faktor pro budoucÃ­ odmÄ›ny
ALPHA: float = 0.001  # Learning rate (rychlost uÄenÃ­)
EPSILON: float = 1.  # PoÄÃ¡teÄnÃ­ hodnota epsilonu pro epsilon-greedy strategii
EPSILON_DECAY: float = 0.99995  # Rychlost sniÅ¾ovÃ¡nÃ­ epsilonu
EPSILON_MIN: float = 0.0001  # MinimÃ¡lnÃ­ hodnota epsilonu
BATCH_SIZE: int = 32  # Velikost batch pÅ™i trÃ©novÃ¡nÃ­
MEMORY_SIZE: int = 10_000  # MaximÃ¡lnÃ­ velikost replay bufferu
FRAME_STACK: int = 4  # PoÄet poslednÃ­ch snÃ­mkÅ¯, kterÃ© agent vidÃ­ souÄasnÄ› pro zachycenÃ­ pohybu v Äase
UPDATE_TARGET_FREQUENCY: int = 10  # PoÄet iteracÃ­ pÅ™ed aktualizacÃ­ target modelu

# SledovÃ¡nÃ­ EXP pro vÃ½poÄet delta_exp (zmÄ›na zkuÅ¡enostÃ­)
prev_exp_value: int = 0

# SledovÃ¡nÃ­ pÅ™edchozÃ­ch hodnot kyslÃ­ku a vody
prev_oxygen: float = 100.
prev_water: float = 100.
oxygen_decrease_counter: float = 1.  # PoÄet snÃ­Å¾enÃ­ kyslÃ­ku
water_decrease_counter: float = 1.  # PoÄet snÃ­Å¾enÃ­ vody


# DDQN Model
# noinspection PyUnresolvedReferences
def build_model() -> tf.keras.Model:
    """
    VytvÃ¡Å™Ã­ hlubokÃ½ neuronovÃ½ model pro Double Deep Q-Network (DDQN).

    Tento model pouÅ¾Ã­vÃ¡ konvoluÄnÃ­ vrstvy pro extrakci vizuÃ¡lnÃ­ch rysÅ¯ z obrazovÃ½ch dat a nÃ¡slednÄ› je
    zpracovÃ¡vÃ¡ pomocÃ­ plnÄ› propojenÃ½ch vrstev pro predikci hodnot Q, kterÃ© odpovÃ­dajÃ­ rÅ¯znÃ½m akcÃ­m.

    Model obsahuje:
    - 2 konvoluÄnÃ­ vrstvy pro extrakci rysÅ¯ z obrazovÃ½ch dat.
    - 1 plnÄ› propojenou vrstvu pro zpracovÃ¡nÃ­ extrahovanÃ½ch rysÅ¯.
    - VÃ½stupnÃ­ vrstvu s poÄtem neuronÅ¯ odpovÃ­dajÃ­cÃ­m poÄtu dostupnÃ½ch akcÃ­.

    :return: KompilovanÃ½ Keras model pÅ™ipravenÃ½ k trÃ©novÃ¡nÃ­.
    """
    model: tf.keras.Sequential = tf.keras.Sequential([
        # 1. konvoluÄnÃ­ vrstva
        # Conv2D - provÃ¡dÃ­ konvoluci na obrazovÃ½ch datech a extrahuje rysy.
        # MÃ¡ 32 filtrÅ¯ (nebo "kernelÅ¯") o velikosti 3x3 pixelÅ¯.
        # Aktivace 'relu' zavÃ¡dÃ­ nelinearitu a zlepÅ¡uje modelovu schopnost uÄit se sloÅ¾itÄ›jÅ¡Ã­ vzory.
        # input_shape urÄuje tvar vstupnÃ­ch dat (12x120xFRAME_STACK), coÅ¾ odpovÃ­dÃ¡ ÄtyÅ™em poslednÃ­m obrazÅ¯m,
        # kterÃ© obsahujÃ­ vizuÃ¡lnÃ­ informace z nÄ›kolika po sobÄ› jdoucÃ­ch snÃ­mkÅ¯.
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(12, 120, FRAME_STACK)),

        # 2. konvoluÄnÃ­ vrstva
        # Conv2D s 64 filtry, velikost 3x3, aktivace 'relu'.
        # Tato vrstva je urÄena pro dalÅ¡Ã­ zpracovÃ¡nÃ­ rysÅ¯ a pomÃ¡hÃ¡ modelu zachytit sloÅ¾itÄ›jÅ¡Ã­ vzory.
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),

        # ZploÅ¡tÄ›nÃ­ (Flatten)
        # Tato vrstva pÅ™evÃ¡dÃ­ 2D vÃ½stupy z pÅ™edchozÃ­ch vrstev na 1D vektor,
        # coÅ¾ umoÅ¾Åˆuje pouÅ¾itÃ­ tÄ›chto vÃ½stupÅ¯ v plnÄ› propojenÃ½ch vrstvÃ¡ch.
        tf.keras.layers.Flatten(),

        # PlnÄ› propojenÃ¡ vrstva (Dense) s 128 neurony
        # KaÅ¾dÃ½ neuron je propojen se vÅ¡emi neurony pÅ™edchozÃ­ vrstvy.
        # Aktivace 'relu' je pouÅ¾ita pro nelineÃ¡rnÃ­ transformaci a umoÅ¾Åˆuje sloÅ¾itÄ›jÅ¡Ã­ rozhodovÃ¡nÃ­.
        tf.keras.layers.Dense(128, activation='relu'),

        # VÃ½stupnÃ­ vrstva
        # Tato vrstva mÃ¡ tolik neuronÅ¯, kolik je dostupnÃ½ch akcÃ­.
        # Aktivace 'linear' znamenÃ¡, Å¾e vÃ½stupnÃ­ hodnoty jsou libovolnÃ© reÃ¡lnÃ© ÄÃ­slo (vhodnÃ© pro hodnoty Q v DQN).
        tf.keras.layers.Dense(ACTION_LEN, activation='linear')
    ])

    # Kompilace modelu
    # Model je kompilovÃ¡n s optimalizÃ¡torem Adam, kterÃ½ je efektivnÃ­ pro trÃ©novÃ¡nÃ­ modelÅ¯ na velkÃ½ch datech.
    # Adam automaticky upravuje rychlost uÄenÃ­ pro kaÅ¾dÃ½ parametr.
    # ZtrÃ¡tovÃ¡ funkce 'mse' (Mean Squared Error) se pouÅ¾Ã­vÃ¡ k minimalizaci rozdÃ­lu mezi pÅ™edpovÄ›zenÃ½mi
    # a skuteÄnÃ½mi hodnotami Q.
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=ALPHA), loss='mse')

    return model


# Funkce pro naÄÃ­tÃ¡nÃ­ modelÅ¯ a epsilonu
# noinspection PyUnresolvedReferences
def load_models() -> tuple[tf.keras.Model, tf.keras.Model]:
    """
    NaÄÃ­tÃ¡ modely pro evaluaci a target model, pokud jiÅ¾ existujÃ­, nebo vytvoÅ™Ã­ novÃ© modely.
    TakÃ© naÄÃ­tÃ¡ hodnotu epsilonu z JSON souboru.

    :return: Tuple obsahujÃ­cÃ­ eval model a target model.
    """
    global EPSILON

    # NaÄÃ­tÃ¡nÃ­ eval modelu
    if os.path.exists(EVAL_MODEL_PATH):
        print("NaÄÃ­tÃ¡m eval model...")
        local_eval_model: tf.keras.Model = tf.keras.models.load_model(EVAL_MODEL_PATH)
    else:
        print("Eval model nenalezen, vytvÃ¡Å™Ã­m novÃ½...")
        local_eval_model: tf.keras.Model = build_model()

    # NaÄÃ­tÃ¡nÃ­ target modelu
    if os.path.exists(TARGET_MODEL_PATH):
        print("NaÄÃ­tÃ¡m target model...")
        local_target_model: tf.keras.Model = tf.keras.models.load_model(TARGET_MODEL_PATH)
    else:
        print("Target model nenalezen, vytvÃ¡Å™Ã­m novÃ½...")
        local_target_model: tf.keras.Model = build_model()
        local_target_model.set_weights(eval_model.get_weights())  # Synchronizace modelÅ¯

    # NaÄÃ­tÃ¡nÃ­ hodnoty epsilonu
    if os.path.exists(EPSILON_PATH):
        with open(EPSILON_PATH) as f:
            EPSILON = json.load(f)  # NaÄtenÃ­ hodnoty epsilonu z JSON souboru
        print(f"NaÄtenÃ¡ hodnota EPSILON: {EPSILON}")
    else:
        print("Soubor s EPSILON nenalezen, pouÅ¾Ã­vÃ¡m vÃ½chozÃ­ hodnotu.")

    return local_eval_model, local_target_model


# NaÄtenÃ­ modelÅ¯ pÅ™i startu
eval_model, target_model = load_models()


# Funkce pro uklÃ¡dÃ¡nÃ­ modelÅ¯ a epsilonu
def save_models() -> None:
    """
    UklÃ¡dÃ¡ aktuÃ¡lnÃ­ modely a hodnotu epsilonu do souborÅ¯.
    Modely se uklÃ¡dajÃ­ do jejich pÅ™Ã­sluÅ¡nÃ½ch cest (EVAL_MODEL_PATH, TARGET_MODEL_PATH),
    a hodnota epsilonu se zapisuje do JSON souboru.

    :return: None
    """
    global eval_model, target_model

    # UloÅ¾enÃ­ modelÅ¯ na disk
    eval_model.save(EVAL_MODEL_PATH)
    target_model.save(TARGET_MODEL_PATH)

    # UloÅ¾enÃ­ hodnoty epsilonu do JSON souboru
    f: SupportsWrite[str]
    with open(EPSILON_PATH, 'w') as f:
        json.dump(EPSILON, f)

    print("Modely a EPSILON uloÅ¾eny.")


# AutomatickÃ© uklÃ¡dÃ¡nÃ­ modelÅ¯ a epsilonu pÅ™i ukonÄenÃ­ programu
atexit.register(save_models)

# PamÄ›Å¥ pro replay buffer (FIFO fronta s omezenou velikostÃ­)
memory: Deque = deque(maxlen=MEMORY_SIZE)

# Å˜Ã­dÃ­cÃ­ promÄ›nnÃ¡ pro ukonÄenÃ­ hlavnÃ­ smyÄky
stop_loop: bool = False

# PromÄ›nnÃ© pro uchovÃ¡vÃ¡nÃ­ poslednÃ­ pÅ™eÄtenÃ© hodnoty EXP a screenshotu
last_exp_img: Optional[np.ndarray] = None
last_exp_value: Optional[str] = None

# Konfigurace oblasti monitoru pro snÃ­mÃ¡nÃ­ obrazovky
monitor: Dict[str, int] = {"top": 0, "left": 0, "width": 1920, "height": 1080}

# FIFO zÃ¡sobnÃ­k pro uchovÃ¡vÃ¡nÃ­ poslednÃ­ch snÃ­mkÅ¯ (frame stack)
frame_stack: Deque[np.ndarray] = deque(maxlen=FRAME_STACK)


def listen_for_key() -> None:
    """
    Funkce ÄekÃ¡ na stisk klÃ¡vesy 'x' a po jejÃ­m stisknutÃ­ ukonÄÃ­ hlavnÃ­ smyÄku.

    :return: None
    """
    global stop_loop

    print("Stiskni 'x' pro ukonÄenÃ­ smyÄky.")
    keyboard.wait('x')  # ÄŒekÃ¡ na stisk klÃ¡vesy 'x'
    print("KlÃ¡vesa 'x' stisknuta, ukonÄuji...")
    stop_loop: bool = True  # NastavÃ­ globÃ¡lnÃ­ promÄ›nnou stop_loop na True, ÄÃ­mÅ¾ ukonÄÃ­ smyÄku


def select_action(state) -> int:
    """
    Vybere akci na zÃ¡kladÄ› aktuÃ¡lnÃ­ho stavu a hodnoty epsilonu.
    Pokud je nÃ¡hodnÃ¡ hodnota menÅ¡Ã­ neÅ¾ epsilon, akce je vybrÃ¡na nÃ¡hodnÄ› podle vÃ¡hy.
    Pokud je nÃ¡hodnÃ¡ hodnota vÄ›tÅ¡Ã­ nebo rovnÃ¡ epsilonu, akce je vybrÃ¡na podle maximÃ¡lnÃ­ hodnoty Q funkce.

    :param state: AktuÃ¡lnÃ­ stav (obvykle ve formÄ› NumPy pole)
    :return: ÄÃ­selnÃ¡ hodnota akce, kterÃ¡ bude vykonÃ¡na
    """
    # Pokud je nÃ¡hodnÃ© ÄÃ­slo menÅ¡Ã­ neÅ¾ epsilon, vybereme nÃ¡hodnou akci s vÃ¡hami
    if np.random.rand() < EPSILON:
        weighted_actions: list = []  # Seznam pro akce s vÃ¡hami
        for action in range(ACTION_LEN):
            # Akce obsahujÃ­cÃ­ 'w' dostanou niÅ¾Å¡Ã­ vÃ¡hu
            if 'w' not in ACTION_MAP[action] or 'wait' in ACTION_MAP[action]:
                # Pokud akce neobsahuje 'w' nebo obsahuje 'wait', pÅ™idÃ¡me akci ÄtyÅ™ikrÃ¡t (vyÅ¡Å¡Ã­ vÃ¡ha)
                weighted_actions.extend([action] * 4)
            else:
                # Jinak pÅ™idÃ¡me akci s vÃ¡hou 1
                weighted_actions.append(action)

        # NÃ¡hodnÃ½ vÃ½bÄ›r akce na zÃ¡kladÄ› vÃ¡hy
        return random.choice(weighted_actions)

    # Pokud je nÃ¡hodnÃ© ÄÃ­slo vÄ›tÅ¡Ã­ neÅ¾ epsilon, vybereme akci s nejvyÅ¡Å¡Ã­ Q hodnotou
    q_values = eval_model(state)  # ZÃ­skÃ¡me Q hodnoty pro danÃ½ stav
    return np.argmax(q_values[0])  # VrÃ¡tÃ­me akci s nejvyÅ¡Å¡Ã­ Q hodnotou


def store_experience(state, action, reward: float, next_state, done: bool) -> None:
    """
    UloÅ¾Ã­ zkuÅ¡enost (stav, akci, odmÄ›nu, nÃ¡sledujÃ­cÃ­ stav, hotovo) do replay bufferu.

    :param state: AktuÃ¡lnÃ­ stav prostÅ™edÃ­ (mÅ¯Å¾e bÃ½t libovolnÃ©ho typu, napÅ™. obraz nebo stav).
    :param action: Akce, kterou agent vykonal (mÅ¯Å¾e bÃ½t libovolnÃ©ho typu, napÅ™. ÄÃ­slo nebo Å™etÄ›zec).
    :param reward: OdmÄ›na za vykonÃ¡nÃ­ akce (float).
    :param next_state: NÃ¡sledujÃ­cÃ­ stav po vykonÃ¡nÃ­ akce (mÅ¯Å¾e bÃ½t libovolnÃ©ho typu, napÅ™. obraz nebo stav).
    :param done: IndikÃ¡tor, zda byla epizoda dokonÄena (True/False).
    :return: None
    """
    memory.append((state, action, reward, next_state, done))


def compute_reward(exp_value: float, oxygen_percentage: float, water_percentage: float) -> float:
    """
    Funkce pro vÃ½poÄet reward na zÃ¡kladÄ› zmÄ›ny EXP, kyslÃ­ku a vody.

    :param exp_value: AktuÃ¡lnÃ­ hodnota EXP
    :param oxygen_percentage: AktuÃ¡lnÃ­ procento kyslÃ­ku
    :param water_percentage: AktuÃ¡lnÃ­ procento vody
    :return: VypoÄtenÃ¡ odmÄ›na (reward) na zÃ¡kladÄ› zmÄ›n v hodnotÃ¡ch
    """
    # GlobÃ¡lnÃ­ promÄ›nnÃ© pro uchovÃ¡nÃ­ pÅ™edchozÃ­ch hodnot
    global prev_exp_value, prev_oxygen, prev_water, oxygen_decrease_counter, water_decrease_counter

    # VÃ½poÄet zmÄ›ny EXP (delta_exp)
    # Pokud je pÅ™edchozÃ­ hodnota EXP None, znamenÃ¡ to, Å¾e se jednÃ¡ o prvnÃ­ iteraci, takÅ¾e zmÄ›na bude 0
    delta_exp: float = 0 if prev_exp_value is None else exp_value - prev_exp_value
    prev_exp_value: float = exp_value  # Aktualizace pro dalÅ¡Ã­ iteraci

    # VÃ½poÄet zmÄ›ny kyslÃ­ku (delta_oxygen) a vody (delta_water)
    # StejnÄ› jako u delta_exp, pokud je pÅ™edchozÃ­ hodnota None, zmÄ›na bude 0
    delta_oxygen: float = 0 if prev_oxygen is None else oxygen_percentage - prev_oxygen
    delta_water: float = 0 if prev_water is None else water_percentage - prev_water

    # Penalizace za opakovanÃ½ Ãºbytek kyslÃ­ku
    if delta_oxygen < 0:
        oxygen_decrease_counter += 0.1  # ZvyÅ¡ujeme penalizaci pÅ™i opakovanÃ©m Ãºbytku kyslÃ­ku
    else:
        oxygen_decrease_counter = 0.1  # Reset penalizace pÅ™i nÃ¡rÅ¯stu kyslÃ­ku

    # Penalizace za opakovanÃ½ Ãºbytek vody
    if delta_water < 0:
        water_decrease_counter += 0.1  # ZvyÅ¡ujeme penalizaci pÅ™i opakovanÃ©m Ãºbytku vody
    else:
        water_decrease_counter = 0.1  # Reset penalizace pÅ™i nÃ¡rÅ¯stu vody

    # Aktualizace pÅ™edchozÃ­ch hodnot pro kyslÃ­k a vodu
    prev_oxygen: float = oxygen_percentage
    prev_water: float = water_percentage

    # Ãšprava penalizace podle poÄtu ÃºbytkÅ¯ kyslÃ­ku a vody
    oxygen_penalty: float = delta_oxygen * oxygen_decrease_counter
    water_penalty: float = delta_water * water_decrease_counter

    # VÃ½slednÃ¡ reward funkce:
    # PouÅ¾Ã­vÃ¡me logaritmickou funkci pro EXP zmÄ›ny a pÅ™idÃ¡vÃ¡me penalizace za Ãºbytky kyslÃ­ku a vody
    reward: float = np.log(max(delta_exp, 1)) / 10.0 + oxygen_penalty + water_penalty

    return reward


def train_ddqn() -> None:
    """
    Funkce pro trÃ©novÃ¡nÃ­ Double Deep Q-Network (DDQN) modelu. TrÃ©nuje model na zÃ¡kladÄ› vzorkÅ¯ z replay bufferu.

    VyuÅ¾Ã­vÃ¡ eval_model pro vÃ½poÄet hodnot Q a target_model pro generovÃ¡nÃ­ hodnoty Q pro target.
    TakÃ© pravidelnÄ› aktualizuje target_model vÃ¡hy na zÃ¡kladÄ› eval_modelu.

    :return: None
    """
    global can_train, is_training

    train_iterations: int = 0  # PoÄet trÃ©novacÃ­ch iteracÃ­
    while not stop_loop:
        if can_train and not is_training:
            with train_lock:
                if is_training:
                    sleep(0.001)
                    continue
                is_training: bool = True  # ZahÃ¡jenÃ­ trÃ©novÃ¡nÃ­

            # Pokud je v pamÄ›ti mÃ¡lo vzorkÅ¯, pÅ™eskoÄÃ­me trÃ©novÃ¡nÃ­
            if len(memory) < BATCH_SIZE:
                with train_lock:
                    is_training: bool = False  # ZastavenÃ­ trÃ©novÃ¡nÃ­
                    continue

            # VytvoÅ™enÃ­ dÃ¡vky (batch) pro trÃ©novÃ¡nÃ­
            batch: tuple[any, any, any, any, any] = random.sample(memory, BATCH_SIZE)
            states, actions, rewards, next_states, dones = zip(*batch)

            # PÅ™edzpracovÃ¡nÃ­ stavu pro trÃ©novÃ¡nÃ­
            states = np.array(states).reshape(-1, 12, 120, FRAME_STACK)
            next_states = np.array(next_states).astype("float32") / 255.0
            next_states = np.squeeze(next_states, axis=1)

            # Predikce Q-hodnot pro stavy a nÃ¡sledujÃ­cÃ­ stavy
            target_qs = eval_model.predict(states, verbose=0)
            next_qs = eval_model.predict(next_states, verbose=0)
            target_qs_next = target_model.predict(next_states, verbose=0)

            # VÃ½poÄet cÃ­lovÃ½ch Q-hodnot pro trÃ©novÃ¡nÃ­
            for i in range(BATCH_SIZE):
                if dones[i]:
                    target_qs[i][actions[i]] = rewards[i]  # Pokud je done, nastavÃ­me odmÄ›nu na hodnotu
                else:
                    best_action: int = np.argmax(next_qs[i])  # NejlepÅ¡Ã­ akce z predikce pro nÃ¡sledujÃ­cÃ­ stav
                    target_qs[i][actions[i]] = rewards[i] + GAMMA * target_qs_next[i][best_action]  # Q-hodnota pro akci

            # TrÃ©novÃ¡nÃ­ eval_modelu
            eval_model.fit(states, target_qs, epochs=1, verbose=0)
            # print("Eval model trained.")

            train_iterations += 1
            if train_iterations % UPDATE_TARGET_FREQUENCY == 0:
                target_model.set_weights(eval_model.get_weights())  # Aktualizace target_modelu na zÃ¡kladÄ› eval_modelu
                # print("Target model updated.")

            with train_lock:
                is_training: bool = False  # ZastavenÃ­ trÃ©novÃ¡nÃ­
            can_train: bool = False  # NastavenÃ­ flagu, Å¾e trÃ©novÃ¡nÃ­ nenÃ­ moÅ¾nÃ©
        else:
            sleep(0.001)  # KrÃ¡tkÃ¡ pauza, pokud nemÅ¯Å¾eme trÃ©novat


def is_valid_exp_value(exp_text: str) -> Optional[float]:
    """
    Funkce kontroluje, zda text reprezentujÃ­cÃ­ zkuÅ¡enostnÃ­ hodnotu (EXP) obsahuje platnou ÄÃ­selnou hodnotu.
    Pokud ano, vrÃ¡tÃ­ ji jako ÄÃ­slo typu float, jinak vrÃ¡tÃ­ None.

    :param exp_text: Å˜etÄ›zec obsahujÃ­cÃ­ hodnotu EXP ve formÃ¡tu "ÄÃ­slo/nÄ›co"
    :return: Pokud je hodnota pÅ™ed '/' platnÃ¡, vrÃ¡tÃ­ ji jako float. Jinak vrÃ¡tÃ­ None.
    """
    # Kontrola, zda exp_text obsahuje '/'
    if "/" in exp_text:
        parts = exp_text.split("/")  # RozdÄ›lÃ­me text na ÄÃ¡sti podle '/'
        try:
            return float(parts[0])  # PokusÃ­me se pÅ™evÃ©st prvnÃ­ ÄÃ¡st na float
        except ValueError:
            return None  # Pokud pÅ™evod selÅ¾e, vrÃ¡tÃ­me None
    return None  # Pokud '/' nenÃ­ pÅ™Ã­tomno, vrÃ¡tÃ­me None


# Funkce pro nastavenÃ­ flagu pro trÃ©novÃ¡nÃ­
def request_training() -> None:
    """
    Funkce nastavÃ­ flag `can_train` na hodnotu True, coÅ¾ signalizuje, Å¾e hlavnÃ­ smyÄka
    poÅ¾aduje spuÅ¡tÄ›nÃ­ trÃ©novÃ¡nÃ­ modelu na jinÃ©m vlÃ¡knÄ›. Tento flag je zkontrolovÃ¡n
    v jinÃ©m vlÃ¡knÄ›, kterÃ© zaÄne trÃ©novat, aniÅ¾ by blokovalo hlavnÃ­ smyÄku.

    :return: None
    """
    global can_train  # OznaÄenÃ­ promÄ›nnÃ© can_train jako globÃ¡lnÃ­, aby bylo moÅ¾nÃ© ji mÄ›nit
    can_train: bool = True  # NastavenÃ­ flagu, Å¾e hlavnÃ­ smyÄka poÅ¾aduje zahÃ¡jenÃ­ trÃ©novÃ¡nÃ­ na jinÃ©m vlÃ¡knÄ›


def log_performance(iteration: int, reward: float, action: int) -> None:
    """
    Funkce pro logovÃ¡nÃ­ vÃ½konu bÄ›hem trÃ©novÃ¡nÃ­.

    :param iteration: PoÄet iteracÃ­ (nebo krokÅ¯) bÄ›hem trÃ©novÃ¡nÃ­.
    :param reward: Hodnota odmÄ›ny zÃ­skanÃ¡ za aktuÃ¡lnÃ­ akci.
    :param action: ÄŒÃ­slo, kterÃ© reprezentuje konkrÃ©tnÃ­ akci provedenou agentem.
    :return: None (funkce nevracÃ­ Å¾Ã¡dnou hodnotu, pouze zapisuje do souboru)
    """
    # OtevÅ™enÃ­ souboru "training_log.txt" v reÅ¾imu pÅ™idÃ¡vÃ¡nÃ­ (append mode)
    with open("training_log.txt", "a") as log_file:
        # ZapsÃ¡nÃ­ informacÃ­ o iteraci, odmÄ›nÄ› a akci do souboru
        log_file.write(f"{iteration}, Reward: {reward:.3f}, Action: {action}\n")


def game_loop() -> None:
    """
    HlavnÃ­ smyÄka pro interakci se hrou, zÃ­skÃ¡vÃ¡nÃ­ obrazÅ¯ a rozhodovÃ¡nÃ­ o akcÃ­ch.

    :return: None
    """
    global last_exp_value, last_exp_img, stop_loop, EPSILON, is_training, prev_water, prev_oxygen

    # NastavenÃ­ Chrome pro maximÃ¡lnÃ­ okno
    chrome_options: Options = Options()
    chrome_options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://evoworld.io")  # OtevÅ™enÃ­ webu
    actions = ActionChains(driver)

    # ÄekÃ¡nÃ­ a objevenÃ­ tlaÄÃ­tka, 3xTAB a enter
    sleep(8)
    for _ in range(3):
        actions.send_keys(Keys.TAB)
    actions.send_keys(Keys.ENTER)
    actions.perform()

    try:
        # ÄŒekÃ¡nÃ­, aÅ¾ se skryje loading bar
        WebDriverWait(driver, 300).until(
            EC.invisibility_of_element_located((By.CLASS_NAME, "loadingBar"))
        )

        # ÄŒekÃ¡nÃ­ na tlaÄÃ­tko pro zaÄÃ¡tek hry a kliknutÃ­ na nÄ›j
        start_game_button = WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '#uiContainer .main .center .gameStartBox .btnStartGame'))
        )
        start_game_button.click()
        print("Kliknuto na start!")
    except Exception as e:
        print(f"Chyba pÅ™i startu: {e}")

    try:
        # ÄŒekÃ¡nÃ­ na tlaÄÃ­tko pro bonus zkuÅ¡enostÃ­ a kliknutÃ­ na nÄ›j
        exp_bonus_button = WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable(
                (By.CSS_SELECTOR, '#uiContainer .popups .popup#join-popup .select-exp-bonus .grey-button'))
        )
        exp_bonus_button.click()
        print("Kliknuto na bonus zkuÅ¡enostÃ­!")
    except Exception as e:
        print(f"Chyba pÅ™i kliknutÃ­ na bonus zkuÅ¡enostÃ­: {e}")

    # ÄŒekÃ¡nÃ­ na naÄtenÃ­ elementu status (oznaÄenÃ­, Å¾e hra bÄ›Å¾Ã­)
    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'status')))
    sleep(0.2)

    loop: int = 0  # PoÄÃ­tadlo smyÄky
    with mss.mss() as sct:  # PouÅ¾itÃ­ mss pro snÃ­mky obrazovky
        while not stop_loop:  # HlavnÃ­ hernÃ­ smyÄka
            loop += 1
            screenshot = sct.grab(monitor)  # SnÃ­mek obrazovky
            image = np.array(screenshot)  # PÅ™evod na NumPy array
            exp_crop = image[149:161, 900:1020]  # OÅ™Ã­znutÃ­ oblasti pro EXP
            processed_image = preprocess_image(exp_crop)  # ZpracovÃ¡nÃ­ obrÃ¡zku

            # Pokud je obrÃ¡zek novÃ½ nebo se zmÄ›nil, zpracuj jej
            if last_exp_img is None or images_are_different(last_exp_img, processed_image):
                last_exp_img = processed_image
                last_exp_value = process_image(processed_image)

            # OÅ™Ã­znutÃ­ pro hladiny kyslÃ­ku a vody
            oxygen_crop = image[166:172, 578:1253]
            oxygen_percentage: float = calculate_fill_percentage(oxygen_crop)
            water_crop = image[185:190, 578:1253]
            water_percentage: float = calculate_fill_percentage(water_crop)

            # Detekce restartu hry a reakce na to
            if detect_and_restart_game(image):
                sleep(2)
                prev_water: float = 100.
                prev_oxygen: float = 100.
                reward: float = -1.  # Penalizace za smrt
                store_experience(state, action, reward, next_state, False)  # UloÅ¾enÃ­ zkuÅ¡enosti se smrtÃ­
                continue

            # PÅ™idÃ¡nÃ­ novÃ©ho zpracovanÃ©ho snÃ­mku do zÃ¡sobnÃ­ku rÃ¡mcÅ¯
            frame_stack.append(processed_image)
            if len(frame_stack) < FRAME_STACK:  # Pokud nenÃ­ dostatek rÃ¡mcÅ¯, pokraÄuj
                continue

            state = np.stack(frame_stack, axis=-1)  # SestavenÃ­ stavu
            state = np.expand_dims(state, axis=0)  # RozÅ¡Ã­Å™enÃ­ dimenzÃ­

            # VÃ½bÄ›r akce na zÃ¡kladÄ› stavu
            action: int = select_action(state)
            keys: str | list[str] = ACTION_MAP[action]  # MapovÃ¡nÃ­ akce na klÃ¡vesy
            execute_action(keys)  # SpuÅ¡tÄ›nÃ­ akce (stisk klÃ¡vesy)

            # Aktualizace stavu a odmÄ›ny
            next_state = np.stack(frame_stack, axis=-1)
            next_state = np.expand_dims(next_state, axis=0)
            reward = compute_reward(
                is_valid_exp_value(last_exp_value) or 0,
                oxygen_percentage,
                water_percentage
            )
            store_experience(state, action, reward, next_state, False)

            # Kontrola, zda je potÅ™eba trÃ©novat
            if loop % 20 == 0 and not is_training:
                request_training()

            # DekrementovÃ¡nÃ­ epsilonu pro exploraci/exploitaci
            if EPSILON > EPSILON_MIN:
                EPSILON *= EPSILON_DECAY

            # log_performance(loop, reward, action)


if __name__ == "__main__":
    # SpuÅ¡tÄ›nÃ­ hernÃ­ smyÄky v samostatnÃ©m vlÃ¡knu a trÃ©novÃ¡nÃ­ DDQN
    threading.Thread(target=listen_for_key).start()
    threading.Thread(target=train_ddqn, daemon=True).start()
    game_loop()


