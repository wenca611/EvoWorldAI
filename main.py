#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Python 3.10+

"""
Projekt: EvoWorld Bot
Autor: Ing. V√°clav Pastu≈°ek
Datum: 2025-03-22
Verze: 1.0.0
Licence: MIT
Popis: Tento skript implementuje DDQN (Double Deep Q-Network) agenta pro hru EvoWorld.io.
Agent se uƒç√≠ hr√°t pomoc√≠ posilovan√©ho uƒçen√≠ (Reinforcement Learning, RL).
Vyu≈æ√≠v√° neuronovou s√≠≈• k odhadu hodnot akƒçn√≠ch strategi√≠ a kombinuje evaluaƒçn√≠ a target s√≠≈•,
co≈æ ≈ôe≈°√≠ probl√©m p≈ôece≈àov√°n√≠ Q-hodnot, typick√Ω pro standardn√≠ DQN.
Agent vn√≠m√° hru jako obrazov√© vstupy, analyzuje je a prov√°d√≠ akce na z√°kladƒõ predikc√≠ sv√© s√≠tƒõ.
Tr√©nuje se s vyu≈æit√≠m Replay Bufferu a Target Network Update, aby se zlep≈°ila stabilita uƒçen√≠.
C√≠lem je dos√°hnout optim√°ln√≠ strategie p≈ôe≈æit√≠ a evoluce v hern√≠m prost≈ôed√≠.
"""

# üõ†Ô∏è Standardn√≠ knihovny Pythonu
import atexit  # Spust√≠ funkci p≈ôi ukonƒçen√≠ programu (nap≈ô. ukl√≠zen√≠ prost≈ôedk≈Ø)
import json  # Pr√°ce s JSON soubory (ukl√°d√°n√≠ a naƒç√≠t√°n√≠ dat)
import os  # P≈ô√≠stup k funkc√≠m operaƒçn√≠ho syst√©mu (soubory, promƒõnn√© prost≈ôed√≠)
import threading  # Pr√°ce s vl√°kny pro soubƒõ≈æn√© prov√°dƒõn√≠ √∫loh
from _typeshed import SupportsWrite  # Typov√Ω hint pro objekty, kter√© podporuj√≠ z√°pis ≈ôetƒõzc≈Ø (str)
from time import sleep  # Pauza v bƒõhu programu (ƒçasov√Ω delay)
import random  # Generov√°n√≠ n√°hodn√Ωch ƒç√≠sel a v√Ωbƒõr prvk≈Ø ze seznam≈Ø
from collections import deque  # Efektivn√≠ struktura FIFO/LIFO (fronta/z√°sobn√≠k)
from typing import Deque, Optional, Dict

# üî¢ Knihovny pro vƒõdeck√© v√Ωpoƒçty a strojov√© uƒçen√≠
import numpy as np  # Numerick√© v√Ωpoƒçty s maticemi a vektory
import tensorflow as tf  # Strojov√© uƒçen√≠ a neuronov√© s√≠tƒõ

# ‚å®Ô∏è Interakce s kl√°vesnic√≠ a obrazovkou
import keyboard  # Zachyt√°v√°n√≠ vstup≈Ø z kl√°vesnice a simulace stisk≈Ø kl√°ves
import mss  # Rychl√© sn√≠m√°n√≠ obrazovky (screenshoty)

# üåê Automatizace webov√©ho prohl√≠≈æeƒçe (Selenium)
from selenium import webdriver  # Hlavn√≠ knihovna pro ovl√°d√°n√≠ prohl√≠≈æeƒçe
from selenium.webdriver import ActionChains, Keys  # Simulace pohybu my≈°i a kl√°ves
from selenium.webdriver.chrome.options import Options  # Konfigurace Chromu
from selenium.webdriver.support.wait import WebDriverWait  # ƒåek√°n√≠ na naƒçten√≠ prvk≈Ø
from selenium.webdriver.support import expected_conditions as EC  # Ovƒõ≈ôov√°n√≠ viditelnosti prvk≈Ø
from selenium.webdriver.common.by import By  # Hled√°n√≠ prvk≈Ø na str√°nce (nap≈ô. By.ID, By.XPATH)

# üìÇ Vlastn√≠ pomocn√© funkce (import z vlastn√≠ho souboru `utils.py`)
from utils import *  # Import v≈°ech funkc√≠ z utils.py (pravdƒõpodobnƒõ vlastn√≠ pomocn√© metody)

# Vypnut√≠ oneDNN pro sn√≠≈æen√≠ varov√°n√≠ p≈ôi inicializaci TensorFlow
os.environ["TF_ENABLE_ONEDNN_OPTS"]: str = "0"

# Cesty k model≈Øm (pro evaluaci a tr√©nov√°n√≠)
EVAL_MODEL_PATH: str = "eval_model.keras"
TARGET_MODEL_PATH: str = "target_model.keras"
EPSILON_PATH: str = 'epsilon.json'

# Mo≈æn√© akce pro agenta (kl√°vesy, kter√© m≈Ø≈æe stisknout)
ACTION_MAP: dict[int, str | list[str]] = {
    0: "a",  # Pohyb doleva
    1: "w",  # Pohyb nahoru
    2: "d",  # Pohyb doprava
    3: "space",  # Skok
    4: ["a", "w"],  # Pohyb doleva + nahoru
    5: ["d", "w"],  # Pohyb doprava + nahoru
    6: "wait"  # ƒåek√°n√≠ (100ms neprov√°d√≠ akci)
}

# Poƒçet dostupn√Ωch akc√≠
ACTION_LEN: int = len(ACTION_MAP)

# ≈ò√≠zen√≠ tr√©nov√°n√≠
is_training: bool = False  # Indikuje, zda pr√°vƒõ prob√≠h√° tr√©nov√°n√≠
train_lock: threading.Lock = threading.Lock()  # Z√°mek pro synchronizaci tr√©nov√°n√≠
can_train: bool = False  # Ovl√°d√°n√≠ spu≈°tƒõn√≠ tr√©nov√°n√≠

# Hyperparametry pro tr√©nov√°n√≠ agenta
GAMMA: float = 0.99  # Diskontn√≠ faktor pro budouc√≠ odmƒõny
ALPHA: float = 0.001  # Learning rate (rychlost uƒçen√≠)
EPSILON: float = 1.  # Poƒç√°teƒçn√≠ hodnota epsilonu pro epsilon-greedy strategii
EPSILON_DECAY: float = 0.99995  # Rychlost sni≈æov√°n√≠ epsilonu
EPSILON_MIN: float = 0.0001  # Minim√°ln√≠ hodnota epsilonu
BATCH_SIZE: int = 32  # Velikost batch p≈ôi tr√©nov√°n√≠
MEMORY_SIZE: int = 10_000  # Maxim√°ln√≠ velikost replay bufferu
FRAME_STACK: int = 4  # Poƒçet posledn√≠ch sn√≠mk≈Ø, kter√© agent vid√≠ souƒçasnƒõ pro zachycen√≠ pohybu v ƒçase
UPDATE_TARGET_FREQUENCY: int = 10  # Poƒçet iterac√≠ p≈ôed aktualizac√≠ target modelu

# Sledov√°n√≠ EXP pro v√Ωpoƒçet delta_exp (zmƒõna zku≈°enost√≠)
prev_exp_value: int = 0

# Sledov√°n√≠ p≈ôedchoz√≠ch hodnot kysl√≠ku a vody
prev_oxygen: float = 100.
prev_water: float = 100.
oxygen_decrease_counter: float = 1.  # Poƒçet sn√≠≈æen√≠ kysl√≠ku
water_decrease_counter: float = 1.  # Poƒçet sn√≠≈æen√≠ vody


# DDQN Model
# noinspection PyUnresolvedReferences
def build_model() -> tf.keras.Model:
    """
    Vytv√°≈ô√≠ hlubok√Ω neuronov√Ω model pro Double Deep Q-Network (DDQN).

    Tento model pou≈æ√≠v√° konvoluƒçn√≠ vrstvy pro extrakci vizu√°ln√≠ch rys≈Ø z obrazov√Ωch dat a n√°slednƒõ je
    zpracov√°v√° pomoc√≠ plnƒõ propojen√Ωch vrstev pro predikci hodnot Q, kter√© odpov√≠daj√≠ r≈Øzn√Ωm akc√≠m.

    Model obsahuje:
    - 2 konvoluƒçn√≠ vrstvy pro extrakci rys≈Ø z obrazov√Ωch dat.
    - 1 plnƒõ propojenou vrstvu pro zpracov√°n√≠ extrahovan√Ωch rys≈Ø.
    - V√Ωstupn√≠ vrstvu s poƒçtem neuron≈Ø odpov√≠daj√≠c√≠m poƒçtu dostupn√Ωch akc√≠.

    :return: Kompilovan√Ω Keras model p≈ôipraven√Ω k tr√©nov√°n√≠.
    """
    model: tf.keras.Sequential = tf.keras.Sequential([
        # 1. konvoluƒçn√≠ vrstva
        # Conv2D - prov√°d√≠ konvoluci na obrazov√Ωch datech a extrahuje rysy.
        # M√° 32 filtr≈Ø (nebo "kernel≈Ø") o velikosti 3x3 pixel≈Ø.
        # Aktivace 'relu' zav√°d√≠ nelinearitu a zlep≈°uje modelovu schopnost uƒçit se slo≈æitƒõj≈°√≠ vzory.
        # input_shape urƒçuje tvar vstupn√≠ch dat (12x120xFRAME_STACK), co≈æ odpov√≠d√° ƒçty≈ôem posledn√≠m obraz≈Øm,
        # kter√© obsahuj√≠ vizu√°ln√≠ informace z nƒõkolika po sobƒõ jdouc√≠ch sn√≠mk≈Ø.
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(12, 120, FRAME_STACK)),

        # 2. konvoluƒçn√≠ vrstva
        # Conv2D s 64 filtry, velikost 3x3, aktivace 'relu'.
        # Tato vrstva je urƒçena pro dal≈°√≠ zpracov√°n√≠ rys≈Ø a pom√°h√° modelu zachytit slo≈æitƒõj≈°√≠ vzory.
        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),

        # Zplo≈°tƒõn√≠ (Flatten)
        # Tato vrstva p≈ôev√°d√≠ 2D v√Ωstupy z p≈ôedchoz√≠ch vrstev na 1D vektor,
        # co≈æ umo≈æ≈àuje pou≈æit√≠ tƒõchto v√Ωstup≈Ø v plnƒõ propojen√Ωch vrstv√°ch.
        tf.keras.layers.Flatten(),

        # Plnƒõ propojen√° vrstva (Dense) s 128 neurony
        # Ka≈æd√Ω neuron je propojen se v≈°emi neurony p≈ôedchoz√≠ vrstvy.
        # Aktivace 'relu' je pou≈æita pro neline√°rn√≠ transformaci a umo≈æ≈àuje slo≈æitƒõj≈°√≠ rozhodov√°n√≠.
        tf.keras.layers.Dense(128, activation='relu'),

        # V√Ωstupn√≠ vrstva
        # Tato vrstva m√° tolik neuron≈Ø, kolik je dostupn√Ωch akc√≠.
        # Aktivace 'linear' znamen√°, ≈æe v√Ωstupn√≠ hodnoty jsou libovoln√© re√°ln√© ƒç√≠slo (vhodn√© pro hodnoty Q v DQN).
        tf.keras.layers.Dense(ACTION_LEN, activation='linear')
    ])

    # Kompilace modelu
    # Model je kompilov√°n s optimaliz√°torem Adam, kter√Ω je efektivn√≠ pro tr√©nov√°n√≠ model≈Ø na velk√Ωch datech.
    # Adam automaticky upravuje rychlost uƒçen√≠ pro ka≈æd√Ω parametr.
    # Ztr√°tov√° funkce 'mse' (Mean Squared Error) se pou≈æ√≠v√° k minimalizaci rozd√≠lu mezi p≈ôedpovƒõzen√Ωmi
    # a skuteƒçn√Ωmi hodnotami Q.
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=ALPHA), loss='mse')

    return model


# Funkce pro naƒç√≠t√°n√≠ model≈Ø a epsilonu
# noinspection PyUnresolvedReferences
def load_models() -> tuple[tf.keras.Model, tf.keras.Model]:
    """
    Naƒç√≠t√° modely pro evaluaci a target model, pokud ji≈æ existuj√≠, nebo vytvo≈ô√≠ nov√© modely.
    Tak√© naƒç√≠t√° hodnotu epsilonu z JSON souboru.

    :return: Tuple obsahuj√≠c√≠ eval model a target model.
    """
    global EPSILON

    # Naƒç√≠t√°n√≠ eval modelu
    if os.path.exists(EVAL_MODEL_PATH):
        print("Naƒç√≠t√°m eval model...")
        local_eval_model: tf.keras.Model = tf.keras.models.load_model(EVAL_MODEL_PATH)
    else:
        print("Eval model nenalezen, vytv√°≈ô√≠m nov√Ω...")
        local_eval_model: tf.keras.Model = build_model()

    # Naƒç√≠t√°n√≠ target modelu
    if os.path.exists(TARGET_MODEL_PATH):
        print("Naƒç√≠t√°m target model...")
        local_target_model: tf.keras.Model = tf.keras.models.load_model(TARGET_MODEL_PATH)
    else:
        print("Target model nenalezen, vytv√°≈ô√≠m nov√Ω...")
        local_target_model: tf.keras.Model = build_model()
        local_target_model.set_weights(eval_model.get_weights())  # Synchronizace model≈Ø

    # Naƒç√≠t√°n√≠ hodnoty epsilonu
    if os.path.exists(EPSILON_PATH):
        with open(EPSILON_PATH) as f:
            EPSILON = json.load(f)  # Naƒçten√≠ hodnoty epsilonu z JSON souboru
        print(f"Naƒçten√° hodnota EPSILON: {EPSILON}")
    else:
        print("Soubor s EPSILON nenalezen, pou≈æ√≠v√°m v√Ωchoz√≠ hodnotu.")

    return local_eval_model, local_target_model


# Naƒçten√≠ model≈Ø p≈ôi startu
eval_model, target_model = load_models()


# Funkce pro ukl√°d√°n√≠ model≈Ø a epsilonu
def save_models() -> None:
    """
    Ukl√°d√° aktu√°ln√≠ modely a hodnotu epsilonu do soubor≈Ø.
    Modely se ukl√°daj√≠ do jejich p≈ô√≠slu≈°n√Ωch cest (EVAL_MODEL_PATH, TARGET_MODEL_PATH),
    a hodnota epsilonu se zapisuje do JSON souboru.

    :return: None
    """
    global eval_model, target_model

    # Ulo≈æen√≠ model≈Ø na disk
    eval_model.save(EVAL_MODEL_PATH)
    target_model.save(TARGET_MODEL_PATH)

    # Ulo≈æen√≠ hodnoty epsilonu do JSON souboru
    f: SupportsWrite[str]
    with open(EPSILON_PATH, 'w') as f:
        json.dump(EPSILON, f)

    print("Modely a EPSILON ulo≈æeny.")


# Automatick√© ukl√°d√°n√≠ model≈Ø a epsilonu p≈ôi ukonƒçen√≠ programu
atexit.register(save_models)

# Pamƒõ≈• pro replay buffer (FIFO fronta s omezenou velikost√≠)
memory: Deque = deque(maxlen=MEMORY_SIZE)

# ≈ò√≠d√≠c√≠ promƒõnn√° pro ukonƒçen√≠ hlavn√≠ smyƒçky
stop_loop: bool = False

# Promƒõnn√© pro uchov√°v√°n√≠ posledn√≠ p≈ôeƒçten√© hodnoty EXP a screenshotu
last_exp_img: Optional[np.ndarray] = None
last_exp_value: Optional[str] = None

# Konfigurace oblasti monitoru pro sn√≠m√°n√≠ obrazovky
monitor: Dict[str, int] = {"top": 0, "left": 0, "width": 1920, "height": 1080}

# FIFO z√°sobn√≠k pro uchov√°v√°n√≠ posledn√≠ch sn√≠mk≈Ø (frame stack)
frame_stack: Deque[np.ndarray] = deque(maxlen=FRAME_STACK)


def listen_for_key() -> None:
    """
    Funkce ƒçek√° na stisk kl√°vesy 'x' a po jej√≠m stisknut√≠ ukonƒç√≠ hlavn√≠ smyƒçku.

    :return: None
    """
    global stop_loop

    print("Stiskni 'x' pro ukonƒçen√≠ smyƒçky.")
    keyboard.wait('x')  # ƒåek√° na stisk kl√°vesy 'x'
    print("Kl√°vesa 'x' stisknuta, ukonƒçuji...")
    stop_loop: bool = True  # Nastav√≠ glob√°ln√≠ promƒõnnou stop_loop na True, ƒç√≠m≈æ ukonƒç√≠ smyƒçku


def select_action(state) -> int:
    """
    Vybere akci na z√°kladƒõ aktu√°ln√≠ho stavu a hodnoty epsilonu.
    Pokud je n√°hodn√° hodnota men≈°√≠ ne≈æ epsilon, akce je vybr√°na n√°hodnƒõ podle v√°hy.
    Pokud je n√°hodn√° hodnota vƒõt≈°√≠ nebo rovn√° epsilonu, akce je vybr√°na podle maxim√°ln√≠ hodnoty Q funkce.

    :param state: Aktu√°ln√≠ stav (obvykle ve formƒõ NumPy pole)
    :return: ƒç√≠seln√° hodnota akce, kter√° bude vykon√°na
    """
    # Pokud je n√°hodn√© ƒç√≠slo men≈°√≠ ne≈æ epsilon, vybereme n√°hodnou akci s v√°hami
    if np.random.rand() < EPSILON:
        weighted_actions: list = []  # Seznam pro akce s v√°hami
        for action in range(ACTION_LEN):
            # Akce obsahuj√≠c√≠ 'w' dostanou ni≈æ≈°√≠ v√°hu
            if 'w' not in ACTION_MAP[action] or 'wait' in ACTION_MAP[action]:
                # Pokud akce neobsahuje 'w' nebo obsahuje 'wait', p≈ôid√°me akci ƒçty≈ôikr√°t (vy≈°≈°√≠ v√°ha)
                weighted_actions.extend([action] * 4)
            else:
                # Jinak p≈ôid√°me akci s v√°hou 1
                weighted_actions.append(action)

        # N√°hodn√Ω v√Ωbƒõr akce na z√°kladƒõ v√°hy
        return random.choice(weighted_actions)

    # Pokud je n√°hodn√© ƒç√≠slo vƒõt≈°√≠ ne≈æ epsilon, vybereme akci s nejvy≈°≈°√≠ Q hodnotou
    q_values = eval_model(state)  # Z√≠sk√°me Q hodnoty pro dan√Ω stav
    return np.argmax(q_values[0])  # Vr√°t√≠me akci s nejvy≈°≈°√≠ Q hodnotou


def store_experience(state, action, reward: float, next_state, done: bool) -> None:
    """
    Ulo≈æ√≠ zku≈°enost (stav, akci, odmƒõnu, n√°sleduj√≠c√≠ stav, hotovo) do replay bufferu.

    :param state: Aktu√°ln√≠ stav prost≈ôed√≠ (m≈Ø≈æe b√Ωt libovoln√©ho typu, nap≈ô. obraz nebo stav).
    :param action: Akce, kterou agent vykonal (m≈Ø≈æe b√Ωt libovoln√©ho typu, nap≈ô. ƒç√≠slo nebo ≈ôetƒõzec).
    :param reward: Odmƒõna za vykon√°n√≠ akce (float).
    :param next_state: N√°sleduj√≠c√≠ stav po vykon√°n√≠ akce (m≈Ø≈æe b√Ωt libovoln√©ho typu, nap≈ô. obraz nebo stav).
    :param done: Indik√°tor, zda byla epizoda dokonƒçena (True/False).
    :return: None
    """
    memory.append((state, action, reward, next_state, done))


def compute_reward(exp_value: float, oxygen_percentage: float, water_percentage: float) -> float:
    """
    Funkce pro v√Ωpoƒçet reward na z√°kladƒõ zmƒõny EXP, kysl√≠ku a vody.

    :param exp_value: Aktu√°ln√≠ hodnota EXP
    :param oxygen_percentage: Aktu√°ln√≠ procento kysl√≠ku
    :param water_percentage: Aktu√°ln√≠ procento vody
    :return: Vypoƒçten√° odmƒõna (reward) na z√°kladƒõ zmƒõn v hodnot√°ch
    """
    # Glob√°ln√≠ promƒõnn√© pro uchov√°n√≠ p≈ôedchoz√≠ch hodnot
    global prev_exp_value, prev_oxygen, prev_water, oxygen_decrease_counter, water_decrease_counter

    # V√Ωpoƒçet zmƒõny EXP (delta_exp)
    # Pokud je p≈ôedchoz√≠ hodnota EXP None, znamen√° to, ≈æe se jedn√° o prvn√≠ iteraci, tak≈æe zmƒõna bude 0
    delta_exp: float = 0 if prev_exp_value is None else exp_value - prev_exp_value
    prev_exp_value: float = exp_value  # Aktualizace pro dal≈°√≠ iteraci

    # V√Ωpoƒçet zmƒõny kysl√≠ku (delta_oxygen) a vody (delta_water)
    # Stejnƒõ jako u delta_exp, pokud je p≈ôedchoz√≠ hodnota None, zmƒõna bude 0
    delta_oxygen: float = 0 if prev_oxygen is None else oxygen_percentage - prev_oxygen
    delta_water: float = 0 if prev_water is None else water_percentage - prev_water

    # Penalizace za opakovan√Ω √∫bytek kysl√≠ku
    if delta_oxygen < 0:
        oxygen_decrease_counter += 0.1  # Zvy≈°ujeme penalizaci p≈ôi opakovan√©m √∫bytku kysl√≠ku
    else:
        oxygen_decrease_counter = 0.1  # Reset penalizace p≈ôi n√°r≈Østu kysl√≠ku

    # Penalizace za opakovan√Ω √∫bytek vody
    if delta_water < 0:
        water_decrease_counter += 0.1  # Zvy≈°ujeme penalizaci p≈ôi opakovan√©m √∫bytku vody
    else:
        water_decrease_counter = 0.1  # Reset penalizace p≈ôi n√°r≈Østu vody

    # Aktualizace p≈ôedchoz√≠ch hodnot pro kysl√≠k a vodu
    prev_oxygen: float = oxygen_percentage
    prev_water: float = water_percentage

    # √öprava penalizace podle poƒçtu √∫bytk≈Ø kysl√≠ku a vody
    oxygen_penalty: float = delta_oxygen * oxygen_decrease_counter
    water_penalty: float = delta_water * water_decrease_counter

    # V√Ωsledn√° reward funkce:
    # Pou≈æ√≠v√°me logaritmickou funkci pro EXP zmƒõny a p≈ôid√°v√°me penalizace za √∫bytky kysl√≠ku a vody
    reward: float = np.log(max(delta_exp, 1)) / 10.0 + oxygen_penalty + water_penalty

    return reward


def train_ddqn() -> None:
    """
    Funkce pro tr√©nov√°n√≠ Double Deep Q-Network (DDQN) modelu. Tr√©nuje model na z√°kladƒõ vzork≈Ø z replay bufferu.

    Vyu≈æ√≠v√° eval_model pro v√Ωpoƒçet hodnot Q a target_model pro generov√°n√≠ hodnoty Q pro target.
    Tak√© pravidelnƒõ aktualizuje target_model v√°hy na z√°kladƒõ eval_modelu.

    :return: None
    """
    global can_train, is_training

    train_iterations: int = 0  # Poƒçet tr√©novac√≠ch iterac√≠
    while not stop_loop:
        if can_train and not is_training:
            with train_lock:
                if is_training:
                    sleep(0.001)
                    continue
                is_training: bool = True  # Zah√°jen√≠ tr√©nov√°n√≠

            # Pokud je v pamƒõti m√°lo vzork≈Ø, p≈ôeskoƒç√≠me tr√©nov√°n√≠
            if len(memory) < BATCH_SIZE:
                with train_lock:
                    is_training: bool = False  # Zastaven√≠ tr√©nov√°n√≠
                    continue

            # Vytvo≈ôen√≠ d√°vky (batch) pro tr√©nov√°n√≠
            batch: tuple[any, any, any, any, any] = random.sample(memory, BATCH_SIZE)
            states, actions, rewards, next_states, dones = zip(*batch)

            # P≈ôedzpracov√°n√≠ stavu pro tr√©nov√°n√≠
            states = np.array(states).reshape(-1, 12, 120, FRAME_STACK)
            next_states = np.array(next_states).astype("float32") / 255.0
            next_states = np.squeeze(next_states, axis=1)

            # Predikce Q-hodnot pro stavy a n√°sleduj√≠c√≠ stavy
            target_qs = eval_model.predict(states, verbose=0)
            next_qs = eval_model.predict(next_states, verbose=0)
            target_qs_next = target_model.predict(next_states, verbose=0)

            # V√Ωpoƒçet c√≠lov√Ωch Q-hodnot pro tr√©nov√°n√≠
            for i in range(BATCH_SIZE):
                if dones[i]:
                    target_qs[i][actions[i]] = rewards[i]  # Pokud je done, nastav√≠me odmƒõnu na hodnotu
                else:
                    best_action: int = np.argmax(next_qs[i])  # Nejlep≈°√≠ akce z predikce pro n√°sleduj√≠c√≠ stav
                    target_qs[i][actions[i]] = rewards[i] + GAMMA * target_qs_next[i][best_action]  # Q-hodnota pro akci

            # Tr√©nov√°n√≠ eval_modelu
            eval_model.fit(states, target_qs, epochs=1, verbose=0)
            # print("Eval model trained.")

            train_iterations += 1
            if train_iterations % UPDATE_TARGET_FREQUENCY == 0:
                target_model.set_weights(eval_model.get_weights())  # Aktualizace target_modelu na z√°kladƒõ eval_modelu
                # print("Target model updated.")

            with train_lock:
                is_training: bool = False  # Zastaven√≠ tr√©nov√°n√≠
            can_train: bool = False  # Nastaven√≠ flagu, ≈æe tr√©nov√°n√≠ nen√≠ mo≈æn√©
        else:
            sleep(0.001)  # Kr√°tk√° pauza, pokud nem≈Ø≈æeme tr√©novat


def is_valid_exp_value(exp_text: str) -> Optional[float]:
    """
    Funkce kontroluje, zda text reprezentuj√≠c√≠ zku≈°enostn√≠ hodnotu (EXP) obsahuje platnou ƒç√≠selnou hodnotu.
    Pokud ano, vr√°t√≠ ji jako ƒç√≠slo typu float, jinak vr√°t√≠ None.

    :param exp_text: ≈òetƒõzec obsahuj√≠c√≠ hodnotu EXP ve form√°tu "ƒç√≠slo/nƒõco"
    :return: Pokud je hodnota p≈ôed '/' platn√°, vr√°t√≠ ji jako float. Jinak vr√°t√≠ None.
    """
    # Kontrola, zda exp_text obsahuje '/'
    if "/" in exp_text:
        parts = exp_text.split("/")  # Rozdƒõl√≠me text na ƒç√°sti podle '/'
        try:
            return float(parts[0])  # Pokus√≠me se p≈ôev√©st prvn√≠ ƒç√°st na float
        except ValueError:
            return None  # Pokud p≈ôevod sel≈æe, vr√°t√≠me None
    return None  # Pokud '/' nen√≠ p≈ô√≠tomno, vr√°t√≠me None


# Funkce pro nastaven√≠ flagu pro tr√©nov√°n√≠
def request_training() -> None:
    """
    Funkce nastav√≠ flag `can_train` na hodnotu True, co≈æ signalizuje, ≈æe hlavn√≠ smyƒçka
    po≈æaduje spu≈°tƒõn√≠ tr√©nov√°n√≠ modelu na jin√©m vl√°knƒõ. Tento flag je zkontrolov√°n
    v jin√©m vl√°knƒõ, kter√© zaƒçne tr√©novat, ani≈æ by blokovalo hlavn√≠ smyƒçku.

    :return: None
    """
    global can_train  # Oznaƒçen√≠ promƒõnn√© can_train jako glob√°ln√≠, aby bylo mo≈æn√© ji mƒõnit
    can_train: bool = True  # Nastaven√≠ flagu, ≈æe hlavn√≠ smyƒçka po≈æaduje zah√°jen√≠ tr√©nov√°n√≠ na jin√©m vl√°knƒõ


def log_performance(iteration: int, reward: float, action: int) -> None:
    """
    Funkce pro logov√°n√≠ v√Ωkonu bƒõhem tr√©nov√°n√≠.

    :param iteration: Poƒçet iterac√≠ (nebo krok≈Ø) bƒõhem tr√©nov√°n√≠.
    :param reward: Hodnota odmƒõny z√≠skan√° za aktu√°ln√≠ akci.
    :param action: ƒå√≠slo, kter√© reprezentuje konkr√©tn√≠ akci provedenou agentem.
    :return: None (funkce nevrac√≠ ≈æ√°dnou hodnotu, pouze zapisuje do souboru)
    """
    # Otev≈ôen√≠ souboru "training_log.txt" v re≈æimu p≈ôid√°v√°n√≠ (append mode)
    with open("training_log.txt", "a") as log_file:
        # Zaps√°n√≠ informac√≠ o iteraci, odmƒõnƒõ a akci do souboru
        log_file.write(f"{iteration}, Reward: {reward:.3f}, Action: {action}\n")


def game_loop() -> None:
    """
    Hlavn√≠ smyƒçka pro interakci se hrou, z√≠sk√°v√°n√≠ obraz≈Ø a rozhodov√°n√≠ o akc√≠ch.

    :return: None
    """
    global last_exp_value, last_exp_img, stop_loop, EPSILON, is_training, prev_water, prev_oxygen

    # Nastaven√≠ Chrome pro maxim√°ln√≠ okno
    chrome_options: Options = Options()
    chrome_options.add_argument("--start-maximized")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://evoworld.io")  # Otev≈ôen√≠ webu
    actions = ActionChains(driver)

    # ƒçek√°n√≠ a objeven√≠ tlaƒç√≠tka, 3xTAB a enter
    sleep(8)
    for _ in range(3):
        actions.send_keys(Keys.TAB)
    actions.send_keys(Keys.ENTER)
    actions.perform()

    try:
        # ƒåek√°n√≠, a≈æ se skryje loading bar
        WebDriverWait(driver, 300).until(
            EC.invisibility_of_element_located((By.CLASS_NAME, "loadingBar"))
        )

        # ƒåek√°n√≠ na tlaƒç√≠tko pro zaƒç√°tek hry a kliknut√≠ na nƒõj
        start_game_button = WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, '#uiContainer .main .center .gameStartBox .btnStartGame'))
        )
        start_game_button.click()
        print("Kliknuto na start!")
    except Exception as e:
        print(f"Chyba p≈ôi startu: {e}")

    try:
        # ƒåek√°n√≠ na tlaƒç√≠tko pro bonus zku≈°enost√≠ a kliknut√≠ na nƒõj
        exp_bonus_button = WebDriverWait(driver, 30).until(
            EC.element_to_be_clickable(
                (By.CSS_SELECTOR, '#uiContainer .popups .popup#join-popup .select-exp-bonus .grey-button'))
        )
        exp_bonus_button.click()
        print("Kliknuto na bonus zku≈°enost√≠!")
    except Exception as e:
        print(f"Chyba p≈ôi kliknut√≠ na bonus zku≈°enost√≠: {e}")

    # ƒåek√°n√≠ na naƒçten√≠ elementu status (oznaƒçen√≠, ≈æe hra bƒõ≈æ√≠)
    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'status')))
    sleep(0.2)

    loop: int = 0  # Poƒç√≠tadlo smyƒçky
    with mss.mss() as sct:  # Pou≈æit√≠ mss pro sn√≠mky obrazovky
        while not stop_loop:  # Hlavn√≠ hern√≠ smyƒçka
            loop += 1
            screenshot = sct.grab(monitor)  # Sn√≠mek obrazovky
            image = np.array(screenshot)  # P≈ôevod na NumPy array
            exp_crop = image[149:161, 900:1020]  # O≈ô√≠znut√≠ oblasti pro EXP
            processed_image = preprocess_image(exp_crop)  # Zpracov√°n√≠ obr√°zku

            # Pokud je obr√°zek nov√Ω nebo se zmƒõnil, zpracuj jej
            if last_exp_img is None or images_are_different(last_exp_img, processed_image):
                last_exp_img = processed_image
                last_exp_value = process_image(processed_image)

            # O≈ô√≠znut√≠ pro hladiny kysl√≠ku a vody
            oxygen_crop = image[166:172, 578:1253]
            oxygen_percentage: float = calculate_fill_percentage(oxygen_crop)
            water_crop = image[185:190, 578:1253]
            water_percentage: float = calculate_fill_percentage(water_crop)

            # Detekce restartu hry a reakce na to
            if detect_and_restart_game(image):
                sleep(2)
                prev_water: float = 100.
                prev_oxygen: float = 100.
                reward: float = -1.  # Penalizace za smrt
                store_experience(state, action, reward, next_state, False)  # Ulo≈æen√≠ zku≈°enosti se smrt√≠
                continue

            # P≈ôid√°n√≠ nov√©ho zpracovan√©ho sn√≠mku do z√°sobn√≠ku r√°mc≈Ø
            frame_stack.append(processed_image)
            if len(frame_stack) < FRAME_STACK:  # Pokud nen√≠ dostatek r√°mc≈Ø, pokraƒçuj
                continue

            state = np.stack(frame_stack, axis=-1)  # Sestaven√≠ stavu
            state = np.expand_dims(state, axis=0)  # Roz≈°√≠≈ôen√≠ dimenz√≠

            # V√Ωbƒõr akce na z√°kladƒõ stavu
            action: int = select_action(state)
            keys: str | list[str] = ACTION_MAP[action]  # Mapov√°n√≠ akce na kl√°vesy
            execute_action(keys)  # Spu≈°tƒõn√≠ akce (stisk kl√°vesy)

            # Aktualizace stavu a odmƒõny
            next_state = np.stack(frame_stack, axis=-1)
            next_state = np.expand_dims(next_state, axis=0)
            reward = compute_reward(
                is_valid_exp_value(last_exp_value) or 0,
                oxygen_percentage,
                water_percentage
            )
            store_experience(state, action, reward, next_state, False)

            # Kontrola, zda je pot≈ôeba tr√©novat
            if loop % 20 == 0 and not is_training:
                request_training()

            # Dekrementov√°n√≠ epsilonu pro exploraci/exploitaci
            if EPSILON > EPSILON_MIN:
                EPSILON *= EPSILON_DECAY

            # log_performance(loop, reward, action)


if __name__ == "__main__":
    # Spu≈°tƒõn√≠ hern√≠ smyƒçky v samostatn√©m vl√°knu a tr√©nov√°n√≠ DDQN
    threading.Thread(target=listen_for_key).start()
    threading.Thread(target=train_ddqn, daemon=True).start()
    game_loop()


